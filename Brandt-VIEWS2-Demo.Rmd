---
title: 'VIEWS Data Setup and Modeling Demo '
author: "Patrick T. Brandt\n"
date: "`r format(Sys.time(), '%B %d, %Y')`"
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
output:
  github_document:
    toc: true
    toc_depth: 2
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 2
    collapsed: yes
    smooth_scroll: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
description: Data and model specification documentation for ViEWS forecasts
bibliography: Forecast.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")

# Check file times to see if we need to re-cache
mtime <- function(file, ...) {
  lapply(Sys.glob(c(file, ...)), function(x) file.info(x)$mtime)
}
```

# Introduction

This demo shows how to read and process the initial data from VIEWS / UCDP.  This is based on the data from [VIEWS](https://viewsforecast.org).  Here the files are downloaded from the Dropbox location for the data.  Alternative version would be direct calls to the [API](https://github.com/prio-data/views_api/wiki/Available-datasets).

# Country-Month Data setup

Here begin with setting up the training data as provided by
[ViEWS](https://viewsforecasting.org/prediction-competition-2/). This
takes the data as given from ViEWS and reads it into several data frames and some subsets.  The next few steps do this initially so that one gets all of the necessary data in one place at the beginning of the analysis that follow.

# Package loads

This section handles loading all packages then used below. This will able be able to allow R Studio to handle in one section any installs / loads / libraries needed below.

```{r loadit, echo=TRUE, message=FALSE}
rm(list=ls())  # Clean start, I know...
library(arrow)
library(curl)
library(zip)
library(plotly)
library(tidyr)
library(ggplot2)
library(statmod)
library(tweedie)
library(MASS)
library(pscl)
library(zoo)
library(scoringutils)
library(magrittr)
#### Taylor diagram ####
library(openair)

```

## Reading in data

This section shows how to automatically download and read in the training data for the VIEWS `cm` to `R`.

*If you run your own local version of this, you may likely need to change the paths to the input files.*  Here are links for the [codebooks](https://viewsforecasting.org/wp-content/uploads/cm_features_competition.pdf) or [here](https://www.dropbox.com/scl/fo/rkj4ttawoz9pv6x35r9cq/ACNAhhWCn6wCJvSv8ZeyMu4/cm_level_data?dl=0&preview=ideas_fastforward_2025_cm_features.pdf&rlkey=44eg0kk4w8yh8tm1f53vvpzps&subfolder_nav_tracking=1) and [input files](https://www.dropbox.com/scl/fo/rkj4ttawoz9pv6x35r9cq/APSd_RJxvk-fpNAteD4J1iY?rlkey=44eg0kk4w8yh8tm1f53vvpzps&e=1&st=r0qv5cz1&dl=0). 

This does take some time initially.  (Once you have these files locally you can bypass this section or alter this to run locally!)

```{r readdata, echo=TRUE, cache=TRUE, cache.rebuild = !file.exists("cm_data_121_542.parquet")}

# CM level data: features and outcomes path
dl_link <- "https://www.dropbox.com/scl/fo/rkj4ttawoz9pv6x35r9cq/ACi53p087OvQUke0pIg88pg/cm_level_data/cm_data_121_542.parquet?rlkey=44eg0kk4w8yh8tm1f53vvpzps&dl=1"

# File name once downloaded
destfile <- "cm_data_121_542.parquet"

# DL with curl
curl::curl_download(url = dl_link, destfile = destfile)

# This is the main data file being read into R
cm <- read_parquet("cm_data_121_542.parquet")

###
## Repeat the above with the data for the country labels and the month ids
##

# Get the list of country names as well to add to the data
# These are from the main VIEWS site
dl_link2 <- "https://www.dropbox.com/scl/fo/rurpcmtpcquni5onoyuus/AGeR6dD-Ru-Emwn06HnKAE8/matching_tables?preview=countries.csv&rlkey=v1o4va647qrwc4la7m8i7cedk&subfolder_nav_tracking=1&st=goucd0hg&dl=1"

# File name declared
destfile2 <- "country.zip"

# DL with curl
curl::curl_download(url = dl_link2, destfile = destfile2)

# Unzip
zip::unzip(zipfile = "country.zip", exdir="countries")

# Read it
countries <- read.csv("countries/countries.csv", header = TRUE)

# Month to date maps
dl_link3 <- "https://www.dropbox.com/scl/fo/rurpcmtpcquni5onoyuus/AGeR6dD-Ru-Emwn06HnKAE8/matching_tables?preview=month_ids.csv&rlkey=v1o4va647qrwc4la7m8i7cedk&subfolder_nav_tracking=1&st=l8g86mv4&dl=1"

destfile3 <- "month_ids.zip"
curl::curl_download(url = dl_link3, destfile = destfile3)
zip::unzip(zipfile = "month_ids.zip", exdir="month_ids")
month_ids <- read.csv("month_ids/month_ids.csv", header = TRUE)

# Save the downloaded data for later -- so this is all in one image!

save.image("VIEWS-alldownloaded.RData")

```

Note the above will also have downloaded the `pgm` items for the months and the countries, so this is available for later processing.  Adding the `pgm` training data would need to be added to the above if desired.  But the basic idea is mapped out here.

## Basic data manipulation for country-months

This section merges or aligns the dates and country codes so that the identifiers can always be put back into the data for case identifications.  This is a set of `merge` commands using base R.

```{r mergething, echo=TRUE}
# Subset out the main (conflict) variables we want
df1 <- subset(cm, select = c(month_id, country_id,
                             gleditsch_ward,
                             ged_sb, ged_ns, ged_os, 
                             acled_sb, acled_sb_count,
                             acled_os))

# Merge on the country label data
dfs <- merge(df1, countries, 
             by.x = "country_id", by.y="id")

# Merge on the time periods info
dfs <- merge(dfs, month_ids[,2:4],
             by.x = "month_id", by.y="month_id")

# Clean up
rm(df1)

```

If you want more variables in a data frame for analysis they can be added as arguments to the `subset()` call.

One can now subset from an object like `dfs` in several ways using all the usual tools.  Just pulling out the data for Libya or by date would be like this:

```{r selector, echo=TRUE}
# Pull out rows for Libya
Libya <- dfs[dfs$isoab=="LBY",]
str(Libya)

# All data since 2022
post21 <- dfs[dfs$Year>2021,]

str(post21)

# Clean ip
rm(post21)
```

Note this gives us what we would expect:

1.  For Libya there are 30 years of monthly data, or 420 observations.
2.  The `post21` object covers 3 years over 12 months for 191 countries: $3 \times 12 \times 191 = 6876$. 


# Basic Time Series Plots of all the SB killings data

This summarizes and plots the GED state-based deaths series in `ged_sb`.  The goal in the next part is to 

1.  extract the data by month
2.  summarize the mean and standard deviation for each month
3.  plot these monthly statistics as time series

```{r demoplots, echo=TRUE, cache=TRUE, fig.keep='all', fig.align='center', fig.height=4}
#### Simple plots as checks ####

# Simple summary plot to see that we have things correct...
ms <- sort(unique(dfs$month_id))

roll.ged.sb <- matrix(NA, nrow=length(ms), ncol=3)
sbdata <- vector(mode="list", length=length(ms))

for(i in 1: length(ms))
{
  # Subset out one month of data at a time
  sb <- dfs[dfs$month_id==ms[i],]$ged_sb 
  sbdata[[i]] <- as.vector(unlist(sb))
  
  # Compute and store monthly means and sd
  roll.ged.sb[i,] <- c(ms[i], mean(sbdata[[i]]), sd(sbdata[[i]]))
}

rm(sb)

# Declare as time series and add variable names
roll.ged.sb <- ts(roll.ged.sb[,2:3], start=c(1990,1), freq=12)
colnames(roll.ged.sb) <- c("Mean SB", "Std.Dev. SB")

# Plot the results
plot(roll.ged.sb, lwd=2, col=1:2, main="", 
     cex.lab=0.9, cex.axis=0.7)
```

# Distributional summaries

Here look at the main variable from UCDP GED, the state-based deaths in the `ged_sb` variable.  (This can be repeated later for the other target measures in the `dfs` object.). Here we compute the summarize the main quantiles from the 75th to the 99th:

```{r distrosums, echo=TRUE, warning=FALSE}
sb_quantiles <- by(dfs$ged_sb, dfs$Year, quantile, 
                   probs=c(0.75,0.85,0.95,0.99))

print(sb_quantiles)
```
Of note here is that the lower 75th to 85th percentile of the distribution is zeros in almost all of the training data.  In recent years this increases slightly.  But this is why baseline predictions of zero or only a recent mean are important for comparison.

Below a time series plot of the same is explored.

## Some key time series summaries of variation

To better see the changes in the data over time, consider the standard deviation time series plot given earlier.  This plot seems to indicate that in more recent years of the training data the variance is larger.  A quick way to see this is to look at the *logarithm* of the standard deviation:

```{r logstd_plot, echo=TRUE, fig.keep='last', fig.align='center', fig.height=4, fig.width=4, fig.cap="Log Standard Deviation in GED state-based"}
plot(log(roll.ged.sb[,2]), ylab="")
```
A further key distributional property of the training data (that should be part of any model) is that the probability of zero events is high (as would be the case given the last section).  To see this, consider

```{r prob0plot, echo=TRUE, fig.keep='last', fig.cap="Frequency of zeros over time", fig.align='center', fig.width=4, fig.height=4}

freq.zeros <- aggregate(dfs$ged_sb==0, by=list(dfs$month_id), mean)

# Poisson predictions make no sense
# range(dpois(0, lambda=roll.ged.sb[,1]))

plot(ts(freq.zeros[,2], start=c(1990,1), freq=12),
     plot.type="single", lwd=2, col=2, ylab = "Pr(ged_sb=0)")

```
Finally an interactive plot of the variation over months can be constructed.  Here the log standard deviation over time in the `ged_sb` are plotted.  The benefit of doing this in the natural logarithmic domain is that then one can see the relative proportionate increases in the variation in more recent years compared to prior years. (The code here will need to be run locally if you would like to see the plot.)

```{r multits, fig.keep='last', warning=FALSE}

# Set up the data for the standard deviations as `x`
x <- roll.ged.sb[,2]

# Months as factor
dmn <- list(month.abb, unique(floor(time(x))))

# convert to data frame by month, to make data retrieval easier
res.x <- as.data.frame(t(matrix(x, 12, dimnames = dmn)))

# set the equal dimension values of the 3d vectors
n <- nrow(res.x)
x.x <- rep(month.abb, times=n)
y.x <- rep(rownames(res.x), each=12)
z.x <- c(log(as.numeric(x))) # note taking logs here.

# May need to append values to the vector
# converted from the time series and we let them
# equal to the last value in the time series so the
# shape of the graph will not be influenced
# n.z <- length(x.us)
# z.us[n.z+1] = z.us[n.z]
# z.us[n.z+2] = z.us[n.z]

# Make into a dataframe
x.us <- data.frame(x.x, y.x, z.x)
colnames(x.us) <- data.frame("x", "y", "z")

# Plot using plot_ly for interactive effects
fig.x <- plot_ly(x.us, x = ~y, y = ~x, z = ~z, 
                 type = 'scatter3d', 
                 mode = 'lines', color=~y.x)

# to turn off the warning caused by the RColorBrewer
suppressWarnings(print(fig.x))

```


## Simple model choices for `cm` data

From the earlier explorations regression-alike prediction models for 'ged_sb` for the `cm` data should have the following properties:

1.  Fit the high probability of zero values on the data
2.  Allow for excess variation (so large predictive variances)
3.  Can alow future covariates like those proposed via the VIEWS codebooks

# Tweedie distribution models

The Tweedie distribution is a member of the exponential dispersion model (EDM) class of distributions.  The canonical references to this are @tweedie1984index which defines the specific distribution, @Jorgensen87 that outlines the EDM class and @jorgensen1997theory that treats the whole class with a specific chapter on the Tweedie distribution.

Numerical results and computational details for the Tweedie distribution and its likeihood estimation are addressed in @zhang2013likelihood, @dunn2008evaluation @dunn2005series, and @dunn2017tweedie.

The reason to consider this distribution for the data is that

1.  It allows a point-mass at zero (without the need for a zero-inflation model or parameters)
2.  It nests some commonly used distrubutions for this kind of problem as special cases (see below)

## Definition

The Tweedie distribution is member of the class of exponential dispersion distribution models [@jorgensen1997theory]. This is a broad class of models that include as special cases normal, gamma, Poisson, binomial, and negative binomial distributions.  A Tweedie random variable has an exponential dispersion for $Y \sim ED(\mu, \sigma^2)$ where 

$$
\begin{align}
E(Y) &= \mu\\
V(Y) &= \sigma^2 \mu^p
\end{align}
$$

This is given as $Y$ follows a Tweedie density with power $p$ of the form $Y \sim TW_p(\mu, \sigma^2)$ and $\theta$ is the canonical paramter of the cumulant of the distribution:

$$
\Pr(Y \in A) = \int_A \exp\left( \frac{\theta \cdot z - \kappa_p(\theta)}{\sigma^2}\right) \nu_\lambda dz
$$
where the cumulant $\kappa()$ of the distribution is a function of $\theta$.  This is a piecewise function of the form 
$$
\kappa_p(\theta)=
\begin{cases}
  \frac{\alpha-1}{\alpha} \left(\frac{\theta}{\alpha-1}\right)^\alpha,  & \text{for }p\neq 1,2\\
  -\log(-\theta), & \text{for }p=2\\
  e^\theta, & \text{for }p=1.
\end{cases}
$$
Typically note the mapping that $\alpha = \frac{p-2}{p-1}$ and $p = \frac{\alpha-2}{\alpha-1}$.   Numerical approximation must be used for evaluation and simulation [See @dunn2005series; @dunn2008evaluation; and @dunn2017tweedie].

The variance function for the ED models is constructed from the canonical mapping of the derivatives of the cumulant for $\kappa()$ to the mean $\mu$.  These are 

$$
\tau(\theta) = \kappa^\prime(\theta) = \mu
$$

When $p$ takes on certain values, the Tweedie reduces to other known exponential distributions [See Sections 4.1 and 4.2 @jorgensen1997theory].  Additional details can be found in [Chapter 5 @jorgensen1997theory].

Specifically these are

| Index        |    Distribution                     |
|:--------------|:--------------------------------|
|$p = 0$       | Normal                           |
|$p \in (0,1)$ | Not defined                      |
|$p = 1$       | Poisson                          |
|$p \in (1,2)$ | Compound Poisson (zero inflated) |
|$p = 2$       | Gamma                            |
|$p = 3$       | inverse-Gaussian                 |

Key idea is that these have a positive mass at zero: so there is just some probability that it is \emph{distinctly} zero that is part of the density.   Additional cases are covered in [@jorgensen1997theory Table 4.1]

## Properties

Tweedie random variables are are subject to scale invariance also known as reproductive exponential dispersion:

$$
  \textrm{Tw} _{p}(\mu ,\sigma ^{2})=\textrm{Tw}_{p}(c\mu ,c^{2-p}\sigma ^{2}).
$$
[@jorgensen1997theory Theorem 4.1]

The variance function has a power-law interpretation as part of it cumulatve generator $\kappa_p(\theta)$ produces 

\begin{eqnarray*}
  \tau (\theta )=\kappa ^{\prime }(\theta )=\mu\\
V(\mu )=\tau ^{\prime }[\tau ^{-1}(\mu )]
\end{eqnarray*}

This formulation is useful because it manifests some well known properties:

- It can model the excess zeros without a separate model (zero inflation) and give them their own mass or probability that matches the data *a priori*. 
- It generalized a negative binomial, or compound Poisson process.
- The process can reproduce versions of a Poisson-gamma convolution to a negative binomial (a common example in Bayesian data analysis of counts).
- It follows a Taylor power law via the variance-to-mean scaling relationship.

## Illustration

To understand the basic properties of the Tweedie model and its key uses in applications, consider the typical case of interest which is a random variable with an atom at zero with a small mean and large variance.  We can do this using a Tweedie distribution over various values of ${\mu,\phi,p}$.  THe following figure gives an illustration for different choices.

```{r densityexample, include=TRUE, warning=FALSE, fig.keep='last', fig.width=8, fig.height=8}

par(mfcol=c(5,5), oma = c(4,1,1,1), mar=c(4,3,1,1))
for(mu in seq(0.5,2,0.5)) {
for(phi in c(seq(1,4,1),10)) # dispersion values loop
{

# Like Jorgensen Figure 4.5
plot(c(-0.1,4), c(0,2), type="n",
     xlab="",  ylab="",
     xaxt="n", yaxt="n", mgp=c(2,.5,0), 
     main= substitute(list(mu, phi) == group("(",list(x,y),")"),
     list(x = mu, y = phi)))
#       bquote(mu == .(mu)))
#     expression(paste(mu)))

axis(1, 0:6, mgp=c(2,.5,0))
axis(2, seq(0,2,0.5), #paste(seq(45,60,5),"%",sep=""),
      mgp=c(2,.5,0))

# Loop over the values of p = power...
for(i in seq(1, 1.9, by=0.3))
    {
curve(dtweedie(x, mu = mu, phi = phi, power = i), 0.01, 4,
      ylab = "", add = TRUE, lwd=2, 
      lty=(i%/%0.3 - 2),
      col=(i%/%0.3 - 2))
  
  # Now add a vertical bar for each Pr(Y=0)
  
}
}
}

# Legend trick
par(fig = c(0, 1, 0, 1), oma = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), new = TRUE)
plot(0, 0, type = 'l', bty = 'n', xaxt = 'n', yaxt = 'n')
legend("bottom", legend = paste("p = ", seq(1, 1.9, by=0.3), sep=""), 
       col=1:4, lty=1:4,
       lwd=2, xpd = TRUE, horiz = TRUE, cex = 1, seg.len=1, bty = 'n')
```

The figure belies the extra mass computed for the probability at zero (cf. Jorgensen 1997, Figure 4.5ff). 

# Tweedie models for '`cm` data

This is a stability check on the data: can we even really generalize an ED like target density?  Do the parameters of the Tweedie fitted model to the data change or stay the same over time (months)?

The reason ask this is that the dynamic representation of the counts and their stability are directly related to the distributions that best fit them.  This point is made in papers by [@BWFP2000] and [@Brandt_Williams_2001] and is a reason to suspect that Poisson, negative binomial and even some zero inflated models may be incorrect for these kinds of data.

The following function and parallel code (it will use up to 8 CPU cores on your machine as written) estimates a Tweedie model for each monthly dataset from 1990-2024. 

```{r rolltw, include=TRUE, cache=TRUE}

oneperiod.fit <- function(y, xi.vec=seq(1.45, 1.775, by=0.005))
{
  out <- tweedie.profile(y~1, xi.vec=xi.vec, 
                         do.plot=FALSE, do.smooth=FALSE,
                         glm.control(maxit=100),
                         method="series")
  return(list(phi = out$phi.max, xi = out$xi.max))
}


# Do explicit parallelization here over nodes.  
# Note this can be made more elegant with the `parallel` package.

library(snow)

nodes <- 8 # Number of cores to use in parallel
cl <- makeCluster(spec=nodes, type="SOCK")
clusterEvalQ(cl, library(tweedie))
clusterSetupRNGstream(cl)
#clusterExport(cl, "xi.vec")
clusterExport(cl, "oneperiod.fit")

# Now fit them all at once!
system.time(all.tw <- clusterApplyLB(cl, sbdata, oneperiod.fit))

stopCluster(cl)
                    
```

# Time series parameter summaries

Plot summaries of the key parameters of the Tweedie models for each time period (month).  These examine the probabilty that there are zeros and how well one can fit the baseline variance of the data using a count model.  

The goals in looking at this are

1.  whether there Tweedie parameters indicate the presence of a compound Poisson or dependent count distribution -- is $p \in (1,2)$.

2.  changes in the $Pr(\textrm{ged_sb}\,=\,0)$ over time.

3.  variance changes that are large or small across time.

Begin by extracting out the fitted model objects from the Tweedie models fit earlier.

```{r summs, include=TRUE, fig.height=6, fig.align='center', fig.keep='last', fig.cap='Tweedie models parameters for each month.  Phi is the dispersion parameter, p is the power parameter', cache=TRUE}
# Now summarize the results
all.tw.sum <- matrix(unlist(all.tw), ncol=2, byrow = TRUE)

# Now make some plots to see what changes over time

twp <- cbind(roll.ged.sb, all.tw.sum)
colnames(twp) <- c("Mean", "Std Dev.", "phi", "p")

# Make a plot to see what changes over time
plot(twp, main="")

```

Now plot the probability of a zero count from the Tweedie, Poisson, and data for comparison.

```{r zeropred, echo=TRUE, fig.height=6, fig.width=6, fig.align="center", fig.keep='last', fig.cap='Probabilty of zero prediction from the Tweedie model compared to baseline frequency of zero.'}

# Convert using Dunn and Smyth paper formula for Pr(Y=0) in this model
twp.convert <- sapply(1:nrow(twp), 
                      function(i) tweedie.convert(xi = twp[i,"p"], 
                                                  mu=twp[i,"Mean"], 
                                                  phi=twp[i,"phi"]),
                     USE.NAMES = FALSE)

twp.convert <- as.data.frame(matrix(unlist(twp.convert), 
                                    ncol=6, byrow=TRUE))
colnames(twp.convert) <- c("poisson.lambda", 
                           "gamma.scale", "gamma.shape",
                           "p0", "gamma.mean", "gamma.phi")

# Compute the observed zeros proportion by month and plot against the predictions:
freq.zeros <- aggregate(dfs$ged_sb==0,
                        by=list(dfs$month_id), mean)

# Poisson predictions make no sense
psn.0 <- dpois(0, lambda=roll.ged.sb[,1])

# Plotting: Tweedie prediction and observed.
par(mfrow=c(2,1))
plot(ts(cbind(twp.convert$p0, freq.zeros[,2]), 
        start=c(1990,1), freq=12),
     plot.type="single", lwd=2, lty=2:3, col=2:1, 
     ylab = "Pr(ged_sb=0)", cex.lab=0.8)

legend(2013, 0.925, 
       c("Observed Frequency", "Tweedie Prediction"), col=2:1,
       lty=2:3, lwd=2, cex=0.75)

plot(1-psn.0, lwd=2, lty=1, ylab="",
     main="Pr(ged_sb=0) from a Poisson distribution", cex.lab=0.8)
```

The Tweedie model captures this important property of the data.  The second graph with the Poisson result shows that using the per-month means misses the probability of the zeros because of the skewed nature of the distribution.


# Simple prediction models

This section shows how to set up some simple baseline prediction models for each country-month in the data.

## Basic count regressions

This section sets up a series of count regression models.  It assumes you split the data into training-test splits and fit the following models:

- P = Poisson 
- NB = Negbin
- ZIP = Zero Inflated Poisson
- ZINB = Zero Inflated Negbin
- TW = Tweedie 

A single unit-specific dummy variable is includes to address country heterogeneity.  This is the pre-specified factor `geo` in the regressions below.

This is all done via the `count.cf` function defined below.  The result is a set of `N` simulated draws from the forecast density for each model and outcome to be predicted in the test data.

```{r basiccounts, echo=TRUE, message=FALSE}

set.seed(145)


# count.cf -- fits a single count model to a series and validates it
# against a training / test setup with forecast metrics
#
# Fits each count regression model on train-test data for
# P = Poisson 
# NB = Negbin
# ZIP = ZI Poisson
# ZINB = ZI Negbin
# TW = Tweedie 
#
# basic model uses the `geo` variable as a covariate.
# 
# After MLE fits, generates and returns N draws from the prediction
# density for each of these models
#

count.cf <- function(train, test, xi.vec = seq(1.1, 1.75, by=0.025), 
                     N = 1000)
{
  # Fit models
  p <- glm(y ~ geo, family = poisson, data=train)
  nb <- glm.nb(y ~ geo, data=train)
  zip <- zeroinfl(y ~ geo, data=train, dist="poisson", model=FALSE, y = FALSE)
  zinb <- zeroinfl(y ~ geo, data=train, dist="negbin", model=FALSE, y = FALSE)
  #  zgeom <- zeroinfl(y ~ 1, data=train, dist="geometric", model=FALSE, y = FALSE)
  
  cat("Fitting Tweedie model grid for xi:\n")  
  param.tw <- tweedie.profile(y ~ geo, xi.vec=xi.vec, 
                              data = train,
                              do.plot=FALSE,
                              control=list(maxit=50),
                              method="series", verbose=FALSE)
  tw <- glm(y ~ geo, data = train,
            family=tweedie(var.power=param.tw$xi.max, link.power=0),
            control=list(maxit=50))
  
  # Predict forecasts for test periods
  n <- nrow(test)
  p.p <- predict(p, newdata=test, type="response")
  p.nb <- predict(nb, newdata=test, type="response")  
  p.zip <- predict(zip, newdata=test, type="response")
  p.zinb <- predict(zinb, newdata=test, type="response")
  #  p.zgeom <- predict(zgeom, newdata=test, type="response")
  p.tw <- predict(tw, newdata=test, type = "response")
  
  # Now simulate the forecasts for each test period
  cat("Sampling forecast densities")
  p.sample <- t(sapply(1:n, function(i) {rpois(N, lambda=p.p[i])}))  
  cat(" .")
  nb.sample <- t(sapply(1:n, function(i) {rnbinom(N, size=nb$theta, mu=p.nb[i])}))
  cat(" .")
  zip.sample <- t(sapply(1:n, function(i) {rpois(N, lambda=p.zip[i])}))
  cat(" .")
  zinb.sample <- t(sapply(1:n, function(i) {rnbinom(N, size=zinb$theta, mu=p.zinb[i])}))  
  cat(" .\n")
  tw.sample <- t(sapply(1:n, function(i) {rtweedie(N, xi=param.tw$xi.max,
                                                   mu=p.tw[i], phi=1) }))
  
  # Some cleanups
  rm(p,nb,zip,zinb,tw,
     p.p,p.nb,p.zip,p.zinb,p.tw)
  
  return(list(P = p.sample,
              NB = nb.sample,
              ZIP = zip.sample,
              ZINB = zinb.sample,
              TW = tw.sample))
}

```

## Demo of the count models over train-test splits

Here a training and test set are established for forecasting and scoring.  The `train` set are the `cm` data from 2021-2023 and the `test` set are the data after January 2024. 

```{r countdemo1, echo=TRUE, warning=FALSE, cache=TRUE}

# Define a DV as a simple variable and a factor for the countries
dfs$y <- dfs$ged_sb
dfs$geo <- as.factor(dfs$country_id)

# Splits and subsets over the relevant periods
train <- dfs[dfs$Year>2020 & dfs$Year < 2024,]
test <- dfs[dfs$Year>2023,]

# Now fit models over the subsets
N <- 100
system.time(count.out <- count.cf(train, test, N=N))

```


The results are in the `count.out` object.  This is list with one set of `N` predictions for each country (191) for each month (12).

```{r strc, echo=TRUE}
cat("Structure of the count.out\n")
str(count.out)

cat("\n\n")

cat("Structure of the test \n")
str(test[,c(1,2,4)])
```

So the row here correspond to the country-months for 2024 and the columns are the `N` predictions.


<!-- ### Count regressions with GAM / splines / GLMMs -->

<!-- This section demonstrates the approach taken in Brandt (2024).   Here an illusration of a basic model with GAM / GLMM components are illustrated and then compared to the earlier section's analyses. -->

# Practical Forecast Scoring

## Basics of Scoring Forecasts

Assumed steps for a forecast target:

1.  Data are split into training and test sets (if you have no true future values).

    1.  Could be future values in time.
    2.  Might be some sampled values
    3.  Cross-validation (CV) or leave-out-observations (LOO) are othere ways.

2.  Use the training data to fit a model and generate predictions over the test set.

3.  Compare the predictions over the observations or cases in the test set to the predictions based on the training set. *This is the scoring task.*

    Key complication: keeping honest about the information that is segmented in the training and test sets.


## Scoring forecasts

Here's how one can compute scoring rules to rank the forecasts from each of the models.  This is akin to [Brandt (n.d.)](https://github.com/PTB-OEDA/VIEWS2-DensityForecasts) which are baseline GAM and GLMM models for the same data (albeit different training-test splits).  The task here is to statistically summaize the comparison of the forecasts to the observed or `test` data for model selection and evaluation.

## Scoring with simple metrics

The goal here is to compare each of the models produced in the last section to the data in the `test` set.  So there are a sample of predictions from each of the five models (labeled  `P`, `NB`, `ZIP`, `ZINB`, and `TW` earlier).  

A simple first comparison is just to look at the summaries of each of the 5 forecast models compared to the observed values for each country-month.  

- Series of plots for the models:
    - scatter
    - QQ (quantile versus quantile), 
    - Empirical cumulative distribution function (ECDF)
- Various metrics for these.

```{r summary_forcs, fig.keep='last', fig.align='center', fig.width=6, fig.height=4, fig.cap="2024 `cm` Mean Poisson Predictions versus Actuals"}
# Poisson means for each country-month merged with `test`
tmp <- data.frame(test, PoissonPred = rowMeans(count.out$P))

par(mfrow=c(1,3))
plot(tmp$PoissonPred, tmp$y, 
     xlab="Mean Poisson Predictions", ylab="Actuals", main = "Scatterplot")
qqplot(tmp$PoissonPred, tmp$y, xlab="Mean Poisson Predictions",
       ylab="Actuals", main = "QQ plot")
plot(ecdf(tmp$PoissonPred), 
     xlab="Mean Poisson Predictions", ylab="Cumulative  Density", main="ECDF")
lines(ecdf(tmp$y), col="red")
legend("bottomright", legend=c("Poisson", "Actual"), col=1:2, lwd=2)

```

The idea here is that if the predictions are good, then for a given model we expcet 

- the scatter plots should be close to a 45-degree line
- the QQ plot should align thw quantiles (deviations or outliers are noted in the tails usually)
- The ECDF should follow the northwester axes of the plot

From the results here there is clear basis for improvement!

## Forecast Formatting

Recall though that one can use the sample of `N = 100` draws of the forecasts for each `cm` to construct various other metrics for performance by units (countries, time or both).

Consider making a dataset of the `N` predictions for the Poisson, Tweedie, and other baseline models for each of the `cm` in 2024.  This is done like this:

```{r merge_cm_pred}
#### Forecasts formatting setup ####
# This is different than in some other applications, but the logic is meant
# to be clear here.

# Add case identifiers to the forecasts for the Poisson and Tweedie models
forecast.P <- cbind(test[,c(1,2)], count.out$P)   # Poisson model ones
forecast.TW <- cbind(test[,c(1,2)], count.out$TW) # Tweedie ones

# Now, reshape these into a `long` format where 
# rows are a cm forecast by model

P.stacked <- reshape(forecast.P, 
                     direction = "long",
                     varying = list(names(forecast.P)[3:(N+2)]),
                     v.names = "predicted",
                     idvar = c("month_id", "country_id"),
                     timevar = "sample_id",
                     times = 1:N)
P.stacked$model <- "Poisson"

TW.stacked <- reshape(forecast.TW, 
                      direction = "long",
                      varying = list(names(forecast.TW)[3:(N+2)]),
                      v.names = "predicted",
                      idvar = c("month_id", "country_id"),
                      timevar = "sample_id",
                      times = 1:N)
TW.stacked$model <- "Tweedie"


# Cleanup
rm(forecast.P, forecast.TW)

# Now merge forecasts with the test data
P.stacked <- merge(P.stacked, test,
                   by.x = c("month_id", "country_id"),
                   by.y = c("month_id", "country_id"))
TW.stacked <- merge(TW.stacked, test,
                    by.x = c("month_id", "country_id"),
                    by.y = c("month_id", "country_id"))

# Make some other baseline forecasts
# All zeros
Zero.stacked <- TW.stacked
Zero.stacked$predicted <- 0
Zero.stacked$model <- "Zeros"

# Common Poisson mean across the training data
Mean.stacked <- TW.stacked
Mean.stacked$predicted <- rpois(nrow(Mean.stacked), mean(test$y))
Mean.stacked$model <- "Training Mean"

# Some reorg for below: here we stack the forecasts into a data.frame
all.stacked <- rbind(P.stacked, TW.stacked, Zero.stacked, Mean.stacked)

# Rename the outcome as `observed` since this is expected in code below.
all.stacked$observed <- all.stacked$y

rm(P.stacked, TW.stacked, Zero.stacked, Mean.stacked)

```

This set of `*.stacked` objects are now the items needed to score the forecasts in terms of RMSE, Brier scores, continuous rank probability scores (CRPS), etc.

The earlier forecast comparisons were for the mean forecasts over the `N` draws of the forecast distribution(s) for each `cm`.  One can look at the stacked set of forecast samples to see how these differ:


```{r summary_forcs_density, fig.keep='last', fig.align='center', fig.width=6, fig.height=4, fig.cap="2024 `cm` Tweedie Density Predictions versus Actuals"}

# Tweedie density for each country-month merged with `test`

par(mfrow=c(1,3))
plot(all.stacked[all.stacked$model=="Tweedie",c(4,22)],  pch=".",
     xlab="Tweedie Predictions", ylab="Actuals", main = "Scatterplot")

qqplot(x = all.stacked[all.stacked$model=="Tweedie",4],
       y = all.stacked[all.stacked$model=="Tweedie",22],
       pch=".",
       xlab="Tweedie Predictions",
       ylab="Actuals", main = "QQ plot")
plot(ecdf(all.stacked[all.stacked$model=="Tweedie",4]), 
     xlab="Tweedie Predictions", 
     ylab="Cumulative  Density", main="ECDF")
lines(ecdf(all.stacked[all.stacked$model=="Tweedie",22]), col="red")
legend("bottomright", legend=c("Tweedie", "Actual"), col=1:2, lwd=2)

```
### Metrics for Forecast evaluation

|   | Meteorology, Finance | Macroeconomics | Conflict Early Warning, Conflict Dynamics |
|------------------|:-----------------|:-----------------|:-----------------|
| Point Metrics | RMSE, MAE | RMSE, MAE | RMSE, MAE, ROC Curves |
| Interval | Scoring Rules | Fan Charts | none |
| Density | VRHs, CRPS PITs, Sharpness Diagrams | LPSs, PITs | none |

Selected Tools for Forecast Evaluation. CRPS, RMSE, MAE, VRH, LPS, and PIT denote Continuous Rank Probability Score, Root Mean Square Error, Mean Absolute Error, Verification Rank Histogram, Log Predictive Score, and Probability Integral Transform, respectively.

### Point forecasts

Consider this figure:

```{r putfig, echo=FALSE, fig.align='center', fig.keep='last'}

plot(density(rnorm(1e6, sd=0.45)), main="", xaxt ="n",
     xlab = "", ylab = "Density", col="red")
lines(density(rnorm(1e6, sd=1)), col="blue")
legend(-1.5, 0.75, c("A", "B"), lty=1, lwd=2, col=c("red","blue"))
#axis(1, 0, "A", col="red")
#axis(1, 1, "B", col="blue")

```

It shows how two analysts (models) could produce the same point (mean) forecast and hence the same prediction error. But, in fact, one analyst, A, assigns much more probability than the other, B, to values of the variable between the modal predictions.

Using point metrics like RMSE leads one to conclude the two analysts perform equally well when, in fact, from a probabilistic standpoint, analyst A is the better "horse.''

### Root MSE

Say the model is $Y_t = \alpha + \beta X_t + \epsilon_t$ with $\epsilon_t \sim N(0, \sigma^2)$ for $t = 1, \ldots, T$.

The estimate of the model's error variance is

$$ 
\hat{s}^2 = \frac{1}{T-2} \sum^{T}_{t=1}(Y_t-\hat{Y_t})^2
$$

where $\widehat{Y}_t$ is the period $t$ prediction.

For the one step ahead point estimate at $T+1$, the estimated forecast error variance is:

$$
\hat{s}^2_f = s^2 \left[1 + \frac{1}{T} +
\frac{(X_{T+1}-\bar{X})^2}{\sum^{T}_{i=1}(X_t-\bar{X})^2}\right]
$$

So we use this last quantity to get the forecast RMSE as $s_f$.

### Problems with RMSE

As a tool for ranking forecasts RMSE has problems (Armstrong and Collopy 1992):

*Lack of reliability:* Rankings of models based on RMSE are uncorrelated across levels of temporal aggregation and time horizons

*Sensitivity to outliers:* RMSE can vary widely if there are outliers in the data.

*Lack of Construct Validity:* RMSE ranks uncorrelated with ranks from other metrics and with consensus ranks.

As indicative of underlying forecast density. Only under some strong assumptions, e.g., underlying DGP is normally distributed. But stylized facts about conflict dynamics suggest underlying density is not normal.

## Calibration & Sharpness

1.  Calibration: statistical consistency between the forecast and observations

2.  Sharpness: the concentration (coverage) of the predictive distribution

The goal of probabilistic forecasting is to achieve a high degree of sharpness subject to calibration (Gneiting et al. 2007). A suite of tools is used to determine whether this goal has been achieved.

How this is done and interpreted is an ongoing research topic, per [Hullman (2024)](https://statmodeling.stat.columbia.edu/2024/11/15/calibration-for-everyone-and-every-decision-maybe/)

## CRPS: measures calibration and sharpness

To address the problems of calibration and sharpness a proper scoring rule is used to summarize the density.  For the kind of forecasts being generated here a good choice of this is a *continuous rank probability score* or CRPS.

-   It measures the squared difference between the total areas of the predicted and observed cumulative distributions

-   Smaller values are better, since it means these distributions are similar

-   It is measured in units of the forecasted variable so you can only compare the CRPS for the same variable

## CRPS formula

Let the forecast variable of interest again be denoted by $x$, the observed value of the variable by $x_a$, and the analyst's (or model's) pdf by $\rho(x)$. The CRPS is $$\begin{aligned}
CRPS(P,x_a)=\displaystyle{\int_{-\infty}^{\infty}}[P(x)-P_a(x)]^2dx,
\end{aligned}$$

where $P$ and $P_a$ are the cumulative distributions:

$$\begin{aligned}
P(x) &= \int_{-\infty}^{x}\rho(y)dy\\
P_a(x) &= H(x-x_a).
\end{aligned}$$

Here, $H$ is the Heaviside function: $H(x-x_a) = 0$ if $(x-x_a)<0$ and $H(x-x_a) = 1$ if $(x-x_a) \ge 0$.

(If that is more calculus than you like, its just a fancy way to count. See the next slide).

## A CRPS Visual

This is an example of what goes into a CRPS calculation for a set of observations scored against their forecast densities.  This is based on the appendix to [@Hegreetal2025](https://doi.org/10.1177/00223433241300862).

![Example from Hegre et al. (2025; Figure 3)](Hegreetal2024-Fig3.png) Notice then that an "aggregate" CRPS is made up of sums of the various grey areas described above.

## CRPS in practice

-   In application, an average of the score often is calculated over this set of forecasts or grid points, $k$: 
$$
    \overline{CRPS} = \sum_{k}w_kCRPS(P_k,x_{a}^{k})
$$ 
where $w_k$ are weights set by the forecaster (typically, $w_k = \frac{1}{k}$).

-   CRPS scores will typically differ from RMSE and MAE forecast metrics rankings. This is because they are scoring the congruence of the full forecast density to the observed data.

-   Forecast scoring or metrics measure how well the forecast (density) compares to the observed data over the test horizon or cases.  

-   There is always some averaging going on here: across geography, times, etc.

## CRPS for `cm` models

This shows how to use the `scoringutils` package functions to score the list of forecasts constructed in `count.out`.

```{r scoringdemo, echo=TRUE, message=FALSE, warning=FALSE}

# Get the mean CRPS values across the country-year-draws
check.crps <- lapply(lapply(count.out, 
                                  crps_sample, 
                                  observed = test$y), mean)

# Show the CRPS for the test data
print(unlist(check.crps), digits = 3)

```

So in this application and example the *negative binomial* models have the best CRPS scores.  

Contrast this with what you would get from using the full densities in the `scoringutils` package. Another example here, but in this one we stack the forecasts into a dataset and label them by model as above:

```{r scored_demo, echo=TRUE, message=FALSE, cache=FALSE}
system.time(scored.out <- score(as_forecast_sample(all.stacked,
                                forecast_unit = c("model", "month_id", "isoab"))))

# Get summaries by correct units of evaluation
library(magrittr)
crps <- scored.out %>% 
  summarise_scores(by=c("model", "month_id"), na.rm=TRUE) 

print(crps[,c(1,2,5)])
```

So this now shows the model-specific CRPS metrics for the time periods.

Additional examples deploying CRPS metrics across models are [here](https://github.com/PTB-OEDA/VIEWS2-DensityForecasts/blob/main/Brandt-VIEWS2-Estimation.Rmd)

Another metric example would be across the countries `se_mean` which is the RMSE:
```{r rmse, echo=TRUE}
rmse <- scored.out %>% 
  summarise_scores(by=c("model", "isoab"), na.rm=TRUE) 

head(rmse[,c(1,2,12)])
str(rmse)
```

This gives back the `rmse` for each country over the time periods for each of the models.  (The illustration only displays the first 6 values.)

## Metrics over time horizons

Here is how to convert the scores from the `scoringutils` functions to a time series:

```{r crps_ts, echo=TRUE, fig.keep='last', warning=FALSE, fig.align='center', fig.width=6, fig.height=4, fig.cap="CRPS Time Series. Lower scores are better!"}
# Make a time series of the CRPS for each model / forecast period
crps.ts <- scored.out %>% arrange(model, month_id)

# Arrange the time series and add names
tmp <- crps.ts %>% summarise_scores(by=c("model", "month_id"))
mnames <- unique(tmp$model, fromLast = TRUE)
crps.ts1 <- matrix(tmp$crps, ncol=length(mnames))
colnames(crps.ts1) <- mnames

crps.ts1 <- ts(crps.ts1, start=c(2024,1), freq=12)

# These are the annualized CRPS to compare to their metrics
# So this is all the years and models.

plot(crps.ts1, plot.type="s", ylab="CRPS Sum", col=1:length(mnames), 
     lty=1:length(mnames), lwd=2)
legend("topleft", legend = mnames, col=1:length(mnames),
       lty=1:length(mnames), lwd=2, cex = 0.65)

```
From this one can see the relative performance of each of the models (distributions) over the test horizon of 2024. The Poisson model is initially worst, followed by the training data mean.  Note how a forecast model of "all zeros" is actually often the best performing one here. 

## Forecasts and metrics on a country basis

TBD

# Additional forecast metrics 

1.  Do the same for other scores and metrics.
2.  Make other relevant subsets of all of this. (What are these?)
3.  Consider diagrams in the Brandt (2024) paper "Evaluating and Scoring in Conflict and Violence Forecasts":
    1.  Taylor Diagrams
    2.  Standard deviation ratios
    3.  Other metrics.

See Dietze, M. C. (2017). [Ecological Forecasting](https://doi.org/10.2307/j.ctvc7796h) Princeton University Press.

## Taylor Diagrams

Taylor diagrams [@Taylor2001] are a visual tool that plot the standardized relative variance of the forecasts to the data at $(1,0)$ in a Cartesian $2^D$ plane.

-   The x-axis is then the relative variance of the data and the y-axis is the relative variance of the forecasts

-   This can also be done for the unstandardized data too.

-   The corresponding circular arcs and the angles of the points for each conditional forecast relative to the data in such a plot then measure the correlation of the two quantities of interest

-   plot demonstrates the root mean squared error differences in the forecasts and the data as a function of their (relative) standard deviations and correlation.

-   This can then be conditioned over the posterior forecast sample attributes (models, time periods, etc.).

## Taylor Diagram math

This figure places the normalized standard deviation of the data on the x-axis and compares it to the normalized standard deviation of the forecasts on the y-axis.

Based on the law of cosines these quantities can be used to relate the correlation and the standard deviation of the data and the forecasts.

For two variables $X$ and $Y$, the centered root mean squared error (CRMSE) is

$$
\begin{aligned}
    CRMSE(X,Y) &= \sqrt{\frac{1}{n} \sum_{i=1}^n [(x_i - \mu_x)(y_i - \mu_y)]^2}\\
    CRMSE(X,Y)^2 &= \sigma_X^2 + \sigma_Y^2  - 2\sigma_X \sigma_Y R_{XY}
\end{aligned}
$$

where $R_{XY}$ is the correlation of $X$ and $Y$.

Using the law of cosines $c^2 = a^2 + b^2 - 2ab \cdot cos(\theta)$, where $\theta =
  arccos(R_{XY})$.

For implementation, see [@ANZEL2023107843] and [@openair].

## Taylor Diagram example code

This uses the `openair` package implementation of the Taylor diagram.

```{r taylordemo, echo=TRUE, fig.keep='last', warning=FALSE, fig.align='center', fig.width=6, fig.height=4, fig.cap="Taylor Diagrams of 2024 `cm` forecast performance"}


# Normalized Taylor diagrams
par(mfrow=c(1,2))
TaylorDiagram(all.stacked,
              obs="observed", mod="predicted",
              group=c("model"), #type = "month_id",
              key.title = "Model",
              normalise=TRUE,
              main = "Normalized Taylor Diagram")

# UN-Normalized Taylor diagram for GLMMs

TaylorDiagram(all.stacked,
              obs="observed", mod="predicted",
              group=c("model"), # type = "year",
              key.title = "Model",
              normalise=FALSE,
              main = "Unnormalized Taylor Diagram")

```

## Standard Deviation Ratios

Ratio of the model prediction standard deviations to the observed. The idea is from @dietze2017ecological.  Here one first computes the forecast standard deviations:

```{r sdratios, echo=TRUE}
#### SD Ratios ####
model.ses <- scored.out %>%
  summarise_scores(by = c("model", "month_id"), na.rm=TRUE) %>%
  group_by(model) %>% arrange(model, month_id)

# Organize and label columns with model names -->
mdls <- unique(model.ses$model) 
sd.model <- matrix(sqrt(model.ses$se_mean), ncol=length(mdls)) 
colnames(sd.model) <- mdls 

```

Next, one constructs time series for these:

```{r, sds_timeseries, echo=TRUE}
# Get the relevant time series setup 
sd.model <- ts(sd.model, start=c(2024,1), freq=12) 
sd.obs <- window(roll.ged.sb[,2], start=c(2024,1), end=c(2024,12)) 
```

Now plot these ratios as time series

```{r sds_ts_plot, echo=TRUE, fig.keep='last', warning=FALSE, fig.align='center', fig.width=6, fig.height=4, fig.cap="Standard deviation ratio plot of 2024 `cm` forecast performance"}
# Plot the SDs
# par(mfcol=c(3,2))
plot(window(sd.model/sd.obs, start=c(2024,1), end=c(2024,12)),
     plot.type = "single", col=1:ncol(sd.model), lwd=2,
     xlab = "Month", ylab = expression(Model[SD]/Data[SD]),
     main="Standard deviation ratios, 2024")
legend("topright", legend = colnames(sd.model), col=1:ncol(sd.model), lwd=2)

print(sd.model/sd.obs, digits=3)

```

<!-- # Now can add / contrast the prediction errors with the data and its -->
<!-- # observed volatility... -->

<!-- par(mfcol=c(2,2), mar=c(4,4,1,1)) -->
<!-- plot(window(roll.ged.sb[,2], start=c(2018,1), end=c(2022,12)), ylab="Mean Counts") -->
<!-- plot(window(sd.model[,4:9]/sd.obs, start=c(2018,1), end=c(2022,12)), -->
<!--      plot.type = "single", col=1:6, lwd=2, -->
<!--      xlab = "Month", ylab = expression(Model[SD]/Data[SD])) -->
<!-- abline(h=1, lty=2) -->
<!-- plot(window(roll.ged.sb[,2], start=c(2023,1), end=c(2023,12)), ylab="Mean Counts") -->
<!-- plot(window(sd.model[,4:9]/sd.obs, start=c(2023,1), end=c(2023,12)), -->
<!--      plot.type = "single", col=1:6, lwd=2, -->
<!--      xlab = "Month", ylab = expression(Model[SD]/Data[SD])) -->
<!-- abline(h=1, lty=2) -->

<!-- ``` -->

## Skill scores

One result needed to score the forecasts are *skill scores*.  Skill scores are a normalization of model scores to some pre-chosen baseline.  So the standard deviation time series in the previous section are a verison of a skill score.  

The `scoringutils` package can automatically generate skill scores across the forecast results.

To get skill scores for say CRPS and RMSE metrics one uses:

```{r skill_scores, echo=TRUE}
get_pairwise_comparisons(scored.out)

get_pairwise_comparisons(scored.out, metric = "crps")

```

NOte these are the "relative" or ratio comparisons of the various scores presented earlier.  So the function mainly handles doing the comparison smartly. 

The same by `month_id`:
```{r skill_scores_paired, echo=TRUE}
get_pairwise_comparisons(scored.out, by="month_id")

```

<!-- ## Setting up other baselines -->

<!-- - Add more of the models to the section where the models are stacked. -->

<!-- - Comparisons to other VIEWS baseline models are addressed [here](https://github.com/PTB-OEDA/VIEWS2-DensityForecasts/blob/main/baseline-views-forecasts.R) -->

# Exercises: `cm` models with covariates

The goal at this point is to begin to explore what happens when various predictor variables are added to the models for the `cm` data.  Since prior work has shown that the [negative binomial and Tweedie models work well](https://viewsforecasting.org/research/prediction-challenge-2023/leaderboard/)

## Exercise 0: Baseline Tweedie model 

Here we return to the original dataset which was the `cm` object (not the `dfs` subset used earlier).

We begin by defining new train-test splits, then building a basic Tweedie regression model with

1.  Fixed effects for the countries
2.  Simple dummy variables for the years

To make the data for this exercise more usable, add the `country_id` and `month_id` variables to the original data (before we only did it for the subsets by variable and time).

```{r exercise-data, echo=TRUE, cache=TRUE}
cm1 <- merge(cm, countries, 
             by.x = "country_id", by.y="id")

# Merge on the time periods info
cm1 <- merge(cm1, month_ids[,2:4],
             by.x = "month_id", by.y="month_id")

# Make factors for countries and years
cm1$country_factor <- as.factor(cm1$isoab)
cm1$year_factor <- as.factor(cm1$Year)

```

In what follows the training data are much bigger than what was done above.  

The splits in the data are

- *Training*: 1990--2022
- *Test*: 2023--2024

```{r baseline-data, echo=TRUE, cache=TRUE}
cm.train <- cm1[cm1$Year < 2023,]
cm.test <- cm1[cm1$Year > 2022,]
```

A baseline Tweedie count model then looks like this for the training data (this will take much longer than what we ran above since it use a much bigger training dataset):

```{r baseline-tweedie, echo=TRUE, warning=FALSE, cache=TRUE}
# Estimate the Tweedie parameter - or not.
param.tw <- tweedie.profile(ged_sb ~ country_factor + year_factor, 
                            xi.vec=seq(1.45, 1.775, by=0.075),
                            data = cm.train,
                            do.plot=FALSE,
                            control=list(maxit=20),
                            method="series",
                            verbose=FALSE)

tw <- glm(ged_sb ~ country_factor + year_factor, 
          data = cm.train,
          family=tweedie(var.power=param.tw$xi.max, link.power=0),
          control=list(maxit=50))
```

Look quickly at the coefficient plots for the fixed effects for the year dummy variables:

```{r coef.plot, echo=TRUE, fig.keep='last'}
coefplot::coefplot(tw, coefficients=paste("year_factor", 1990:2022, sep=""),
                   shorten=TRUE)

```

Now we use the fitted model to generate predictions over the test data using the `cm.test` object in the predictions.

```{r baseline-prediction, echo=TRUE}
# Predictions
# Set the factor values for factor_year to be the same as 2022!
cm.test$year_factor <- as.factor(2022)
p.tw <- predict(tw, newdata=cm.test, type = "response")
  
# Now simulate the forecasts for each test period
# Recall: N was set above
tw.sample <- t(sapply(1:length(p.tw), function(i) {rtweedie(N, 
                                                            xi=param.tw$xi.max,
                                                 mu=p.tw[i], phi=1) }))

```

Repeat the forecast formatting like earlier:

```{r reformat, echo=TRUE}

# Add case idtenifiers to the forecasts for the PoissonTweedie models
forecast.TW <- cbind(cm.test[,c(1,2)], tw.sample) # Tweedie ones

# Now, reshape these into a `long` format where 
# rows are a cm forecast by model

TW.stacked <- reshape(forecast.TW, 
                      direction = "long",
                      varying = list(names(forecast.TW)[3:(N+2)]),
                      v.names = "predicted",
                      idvar = c("month_id", "country_id"),
                      timevar = "sample_id",
                      times = 1:N)
TW.stacked$model <- "Tweedie2"

# Cleanup
rm(forecast.TW)

# Now merge forecasts with the test data
keep.variables <- c("month_id", "country_id", "ged_sb", "isoname", "isoab", "isonum")

TW.stacked <- merge(TW.stacked, 
                    subset(cm.test, select = keep.variables),
                    by.x = c("month_id", "country_id"),
                    by.y = c("month_id", "country_id"))

# Make some other baseline forecasts
# All zeros
Zero.stacked <- TW.stacked
Zero.stacked$predicted <- 0
Zero.stacked$model <- "Zeros"

# Common Poisson mean across the training data
Mean.stacked <- TW.stacked
Mean.stacked$predicted <- rpois(nrow(Mean.stacked), mean(cm.test$ged_sb))
Mean.stacked$model <- "Training Mean"

# Some reorg for below: here we stack the forecasts into a data.frame
allnew.stacked <- rbind(TW.stacked, Zero.stacked, Mean.stacked)

# Rename the outcome as `observed` since this is expected in code below.
allnew.stacked$observed <- allnew.stacked$ged_sb

rm(TW.stacked, Zero.stacked, Mean.stacked)

```

Then we can score these predictions:

```{r score_example, echo=TRUE}
Tweedie.scored <- score(as_forecast_sample(allnew.stacked,
                        forecast_unit = c("model", "month_id", "isoab")))

get_pairwise_comparisons(Tweedie.scored, metric = "crps")

Tweedie.scored %>% summarise_scores(by = "model")
```

It is really hard to beat `Zeros`!

## Exercise 1: Climate variables

1.  Extend the baseline model in the previous section with say rainfall or some other climate measures.

2.  Then score the model against the baselines in the previous section.

## Exercise 2: Demography

1.  Use a demographic covariate in a model like log(IMR).

2.  Then score the model against the baselines in the previous section.

Key variables here from the codebook could be:

````{verbatim}
wdi_sp_dyn_le00_in : Life expectancy at birth, total year
wdi_sp_dyn_imrt_in : Infant mortality rate per 1,000 live births.
wdi_sh_dyn_mort_fe : Female under-five mortality rate per 1,000 live birth.
wdi_sp_pop_14_fe_zs : Female population between the ages 0 to 14 as a percentage of the total female population
wdi_sp_pop_1564_fe_zs : Female population between the ages 15 to 64 as a percentage of the total female population.
wdi_sp_pop_65up_fe_zs : Female population 65 years of age or older as a percentage of the total female population.
wdi_sp_pop_grow : Annual population growth in percentage.
wdi_sp_urb_totl_in_zs : Percentage of population living in urban population areas.
splag_wdi_sl_tlf_totl_fe_zs : Female labor force as a percentage of the total.
splag_wdi_sm_pop_refg_or : Refugee population by country or territory of origin.
splag_wdi_sm_pop_netm : Net total of migrants during a five-year estimate.
````

## Exercise 3: Civil-Military Expenditures

1.  Use measures of military expenditures or fiscal pressures as predictors.

2.  Then score the model against the baselines in the previous section.

Key variables here from the codebook could be:
````{verbatim}
wdi_ms_mil_xpnd_gd_zs : Military expenditure as percentage of GDP.

wdi_ms_mil_xpnd_zs : Military expenditure as percentage of general government expenditure.

vdem_v2x_ex_military : Military dimension index on an interval from low to high (0-1).


````

```{r civmil_exp, echo=TRUE}
```
## Exercise 4: Extract out predictions by region or country

This is a quick way to see how scores can be extracted by country for a time period:

```{r extract_scores}
Tweedie.scored %>% filter(isoab=="LBY") %>% summarise_scores(by="month_id")

```


1.  Visualize the above.

2.  Plot the forecast densities over the training and test periods for a case like this.

# References


<!-- ```{r echo=FALSE} -->
<!-- # Run this to get a saved image of everything! -->
<!-- save.image("Brandt-VIEWS-Demo.RData") -->
<!-- ``` -->

