---
title: 'VIEWS Data Setup and Modeling Demo '
author: "Patrick T. Brandt\n"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  github_document:
    toc: true
    toc_depth: 2
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 2
    collapsed: yes
    smooth_scroll: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
description: Data and model specification documentation for ViEWS2 forecasts
bibliography: Forecast.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")

# Check file times to see if we need to re-cache
mtime <- function(file, ...) {
  lapply(Sys.glob(c(file, ...)), function(x) file.info(x)$mtime)
}
```

# Introduction

This demo shows how to read and process the initial data from VIEWS / UCDP.  This is based on the data from [VIEWS](https://viewsforecast.org).  Here the files are downloaded from the Dropbox location for the data.  Alternative version would be direct calls to the [API](https://github.com/prio-data/views_api/wiki/Available-datasets).

# Country-Month Data setup

Here begin with setting up the training data as provided by
[ViEWS](https://viewsforecasting.org/prediction-competition-2/). This
takes the data as given from ViEWS and reads it into several data frames
and some subsets.

### Reading in data

Now these are compressed, which means there are only data for all
observations that are observed.

*If you run your own version of this, you will likely need to change the paths to the input files.*  Here are links for the [codebooks](https://viewsforecasting.org/wp-content/uploads/cm_features_competition.pdf) or [here](https://www.dropbox.com/scl/fo/rkj4ttawoz9pv6x35r9cq/ACNAhhWCn6wCJvSv8ZeyMu4/cm_level_data?dl=0&preview=ideas_fastforward_2025_cm_features.pdf&rlkey=44eg0kk4w8yh8tm1f53vvpzps&subfolder_nav_tracking=1) and [input files](https://www.dropbox.com/scl/fo/rkj4ttawoz9pv6x35r9cq/APSd_RJxvk-fpNAteD4J1iY?rlkey=44eg0kk4w8yh8tm1f53vvpzps&e=1&st=r0qv5cz1&dl=0). 

```{r readdata, echo=TRUE, cache=TRUE, cache.rebuild = !file.exists("cm_data_121_542.parquet")}

# CM level data: features and outcomes
dl_link <- "https://www.dropbox.com/scl/fo/rkj4ttawoz9pv6x35r9cq/ACi53p087OvQUke0pIg88pg/cm_level_data/cm_data_121_542.parquet?rlkey=44eg0kk4w8yh8tm1f53vvpzps&dl=1"

destfile <- "cm_data_121_542.parquet"

# DL with curl
curl::curl_download(url = dl_link, destfile = destfile)

# For the parquet files load the `arrow` package
library(arrow)

# This is the main data file
cm <- read_parquet("cm_data_121_542.parquet")

# Get the list of country names as well to add to the data
# These are from the main VIEWS site
dl_link2 <- "https://www.dropbox.com/scl/fo/rurpcmtpcquni5onoyuus/AGeR6dD-Ru-Emwn06HnKAE8/matching_tables?preview=countries.csv&rlkey=v1o4va647qrwc4la7m8i7cedk&subfolder_nav_tracking=1&st=goucd0hg&dl=1"

destfile2 <- "country.zip"

# DL with curl
curl::curl_download(url = dl_link2, destfile = destfile2)

# Unzip
zip::unzip(zipfile = "country.zip", exdir="countries")

# Read it
countries <- read.csv("countries/countries.csv", header = TRUE)

# Month to date maps
dl_link3 <- "https://www.dropbox.com/scl/fo/rurpcmtpcquni5onoyuus/AGeR6dD-Ru-Emwn06HnKAE8/matching_tables?preview=month_ids.csv&rlkey=v1o4va647qrwc4la7m8i7cedk&subfolder_nav_tracking=1&st=l8g86mv4&dl=1"

destfile3 <- "month_ids.zip"
curl::curl_download(url = dl_link3, destfile = destfile3)
zip::unzip(zipfile = "month_ids.zip", exdir="month_ids")
month_ids <- read.csv("month_ids/month_ids.csv", header = TRUE)

```

(Note the above will also have downloaded the `pgm` items for the months and the countries, so this is available for later processing.  Adding the `pgm` training data would need to be added to the above if desired.)

## Basic data manipulation for country-months

Here we see how to align the dates and country codes so that the identifiers can always be put back into the data for case identifications.  This is a set of `merge` commands using base R.

```{r mergething, echo=TRUE}

# Subset out the main (conflict) variables we want
df1 <- subset(cm, select = c(month_id, country_id,
                             gleditsch_ward,
                             ged_sb, ged_ns, ged_os, 
                             acled_sb, acled_sb_count,
                             acled_os))

# Merge on the country label data
dfs <- merge(df1, countries, 
             by.x = "country_id", by.y="id")

# Merge on the time periods info
dfs <- merge(dfs, month_ids[,2:4],
             by.x = "month_id", by.y="month_id")

# Clean up
rm(df1)

```


# Basic Time Series Plots of all the SB killings data

This summarizes and plots the GED state-based (SB) deaths series.  The goal in the next part is to 

1.  extract the data by month
2.  sumamrize the mean and standard deviation for each month
3.  Plot these monthly statistics as time series

```{r demoplots, echo=TRUE, cache=TRUE, fig.keep='all', fig.align='center', fig.height=4}
#### Simple plots as checks ####

# Simple summary plot to see that we have things correct...
ms <- sort(unique(dfs$month_id))

roll.ged.sb <- matrix(NA, nrow=length(ms), ncol=3)
sbdata <- vector(mode="list", length=length(ms))

for(i in 1: length(ms))
{
  # Subset out one month of data at a time
  sb <- dfs[dfs$month_id==ms[i],]$ged_sb 
  sbdata[[i]] <- as.vector(unlist(sb))
  
  # Compute and store monthly means and sd
  roll.ged.sb[i,] <- c(ms[i], mean(sbdata[[i]]), sd(sbdata[[i]]))
}

rm(sb)

# Declare as time series and add variable names
roll.ged.sb <- ts(roll.ged.sb[,2:3], start=c(1990,1), freq=12)
colnames(roll.ged.sb) <- c("Mean SB", "Std.Dev. SB")

# Plot the results
plot(roll.ged.sb, lwd=2, col=1:2, main="", 
     cex.lab=0.9, cex.axis=0.7)
```

# Distributional summaries

Here we look at the main variable from GED, the state-based deaths in the `ged_sb` variable.  (This can be repeated later for the other target measures in the `dfs` object.). Here we compute the summarize the main quantiles from the 75th to the 99th:

```{r distrosums, echo=TRUE, warning=FALSE}
sb_quantiles <- by(dfs$ged_sb, dfs$Year, quantile, 
                   probs=c(0.75,0.85,0.95,0.99))

print(sb_quantiles)
```
Of note here is that the lower 75th to 8th percentile of the distribution is zeros in almost all of the training data.  In recent years this increases slightly.  But this is why baseline predictions of zero or only a recent mean and important for comparison

### Some key time series summaries of variation

To better see the changes in the data over time, consider the standard deviation time series plot given earlier.  This plot seems to indicate that in more recent years of the training data the variance is larger.  A quick way to see this is to look at the *logarithm* of the standard deviation:

```{r logstd_plot, echo=TRUE, fig.keep='last', fig.align='center', fig.height=4, fig.width=4, fig.cap="Log Standard Deviation in GED state-based"}
plot(log(roll.ged.sb[,2]), ylab="")
```
A further key distributional property of the training data (that should be part of any model) is that the probability of zero events is high (as would be the case given the last section).  To see this, consider

```{r prob0plot, echo=TRUE, fig.keep='last', fig.cap="Frequency of zeros over time", fig.align='center', fig.width=4, fig.height=4}

freq.zeros <- aggregate(dfs$ged_sb==0, by=list(dfs$month_id), mean)

# Poisson predictions make no sense
# range(dpois(0, lambda=roll.ged.sb[,1]))

plot(ts(freq.zeros[,2], start=c(1990,1), freq=12),
     plot.type="single", lwd=2, col=2, ylab = "Pr(ged_sb=0)")

```
Finally an interactive plot of the variation over months can be constructed as follows:

```{r multits, fig.keep='last'}
library(plotly)
library(tidyr)
library(ggplot2)

x <- roll.ged.sb[,2]
dmn <- list(month.abb, unique(floor(time(x))))
# convert to data frame by month, to make data retrieval easier
res.x <- as.data.frame(t(matrix(x, 12, dimnames = dmn)))
# set the values of the 3d vectors
n <- nrow(res.x)
x.x <- rep(month.abb, times=n)
y.x <- rep(rownames(res.x), each=12)
z.x <- c(log(as.numeric(x)))

# May need to append values to the vector
# converted from the time series and we let them
# equal to the last value in the time series so the
# shape of the graph will not be influenced
# n.z <- length(x.us)
# z.us[n.z+1] = z.us[n.z]
# z.us[n.z+2] = z.us[n.z]
x.us <- data.frame(x.x, y.x, z.x)
colnames(x.us) <- data.frame("x", "y", "z")
fig.x <- plot_ly(x.us, x = ~y, y = ~x, z = ~z, 
                 type = 'scatter3d', 
                 mode = 'lines', color=~y.x)

# to turn off the warning caused by the RColorBrewer
suppressWarnings(print(fig.x))

```


## Simple model choices for `cm` data

From the earlier explorations regression-alike prediction models for 'ged_sb` for the `cm` data should have the following properties:

1.  Fit the high probability of zero values on the data
2.  Allow for excess variation (so large predictive variances)
3.  Can alow future covariates like those proposed via the VIEWS codebooks.

# Tweedie distribution models

The Tweedie distribution is a member of the exponential dispersion model (EDM) class of distributions.  The canonical references to this are @tweedie1984index which defines the specific distribution, @Jorgensen87 that outlines the EDM class and @jorgensen1997theory that treats the whole class with a specific chapter on the Tweedie distribution.

Numerical results and computational details for the Tweedie distribution and its likeihood estimation are addressed in @zhang2013likelihood, @dunn2008evaluation @dunn2005series, and @dunn2017tweedie.

## Definition

The Tweedie distribution is member of the class of exponential dispersion distribution models [@jorgensen1997theory]. This is a broad class of models that include as special cases normal, gamma, Poisson, binomial, and negative binomial distributions.  A Tweedie random variable has an exponential dispersion for $Y \sim ED(\mu, \sigma^2)$ where 

$$
\begin{align}
E(Y) &= \mu\\
V(Y) &= \sigma^2 \mu^p
\end{align}
$$

This is given as $Y$ follows a Tweedie density with power $p$ of the form $Y \sim TW_p(\mu, \sigma^2)$:

$$
\Pr(Y|\mu, \sigma^2) = \int_A \exp\left( \frac{\theta \cdot z - \kappa_p(\theta)}{\sigma^2}\right) \nu_\lambda dz
$$
where
$$
\kappa_p(\theta)=
\begin{cases}
  \frac{\alpha-1}{\alpha} \left(\frac{\theta}{\alpha-1}\right)^\alpha,  & \text{for }p\neq 1,2\\
  -\log(-\theta), & \text{for }p=2\\
  e^\theta, & \text{for }p=1
\end{cases}
$$
Typically note the mapping that $\alpha = \frac{p-2}{p-1}$ and $p = \frac{\alpha-2}{\alpha-1}$.   Numerical approximation must be used for evaluation and simulation [See @dunn2005series; @dunn2008evaluation; and @dunn2017tweedie].

When $p$ takes on certain values, the Tweedie reduces to other known exponential distributions [See Sections 4.1 and 4.2 @jorgensen1997theory].  

Specifically these are

| Index        |    Distribution                     |
|:--------------|:--------------------------------|
|$p = 0$       | Normal                           |
|$p \in (0,1)$ | Not defined                      |
|$p = 1$       | Poisson                          |
|$p \in (1,2)$ | Compound Poisson (zero inflated) |
|$p = 2$       | Gamma                            |
|$p = 3$       | inverse-Gaussian                 |

Key idea is that these have a positive mass at zero: so there is just some probability that it is \emph{distinctly} zero that is part of the density.   Additional cases are covered in [@jorgensen1997theory Table 4.1]

## Properties

Tweedie random variables are are subject to scale invariance also known as reproductive exponential dispersion:

$$
  c\operatorname {Tw} _{p}(\mu ,\sigma ^{2})=\operatorname {Tw} _{p}(c\mu ,c^{2-p}\sigma ^{2}).
$$
[@jorgensen1997theory Theorem 4.1]

The variance function has a power-law interpretation as part of it cumulatve generator $\kappa_p(\theta)$ produces 

\begin{eqnarray*}
  \tau (\theta )=\kappa ^{\prime }(\theta )=\mu\\
V(\mu )=\tau ^{\prime }[\tau ^{-1}(\mu )]
\end{eqnarray*}

This formulation is useful because it manifests some well known properties:
\begin{itemize}
\item It can model the excess zeros without a separate model (zero inflation) and give them their own mass or probability that matches the data \emph{a priori}. 
\item It generalized a negative binomial, or compound Poisson process.
\item The process can reproduce versions of a Poisson-gamma convolution to a negative binomial (a common example in Bayesian data analysis of counts).
\item It follows a Taylor power law via the variance-to-mean scaling relationship.
\end{itemize}

## Illustration

To understand the basic properties of the model and its key uses in applications, consider the typical case of interest which is a random variable with an atom at zero with a small mean and large variance.  We can do this using a Tweedie distribution over various values of ${\mu,\phi,p}$.  THe following figure gives an illustration for different choices.

```{r densityexample, include=TRUE, warning=FALSE, fig.keep='last', fig.width=8, fig.height=8}
library(statmod)
library(tweedie)

par(mfcol=c(5,5), oma = c(4,1,1,1), mar=c(4,3,1,1))
for(mu in seq(0.5,2,0.5)) {
for(phi in c(seq(1,4,1),10)) # dispersion values loop
{

# Like Jorgensen Figure 4.5
plot(c(-0.1,4), c(0,2), type="n",
     xlab="",  ylab="",
     xaxt="n", yaxt="n", mgp=c(2,.5,0), 
     main= substitute(list(mu, phi) == group("(",list(x,y),")"),
     list(x = mu, y = phi)))
#       bquote(mu == .(mu)))
#     expression(paste(mu)))

axis(1, 0:6, mgp=c(2,.5,0))
axis(2, seq(0,2,0.5), #paste(seq(45,60,5),"%",sep=""),
      mgp=c(2,.5,0))

# Loop over the values of p = power...
for(i in seq(1, 1.9, by=0.3))
    {
curve(dtweedie(x, mu = mu, phi = phi, power = i), 0.01, 4,
      ylab = "", add = TRUE, lwd=2, 
      lty=(i%/%0.3 - 2),
      col=(i%/%0.3 - 2))
  
  # Now add a vertical bar for each Pr(Y=0)
  
}
}
}

# Legend trick
par(fig = c(0, 1, 0, 1), oma = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), new = TRUE)
plot(0, 0, type = 'l', bty = 'n', xaxt = 'n', yaxt = 'n')
legend("bottom", legend = paste("p = ", seq(1, 1.9, by=0.3), sep=""), 
       col=1:4, lty=1:4,
       lwd=2, xpd = TRUE, horiz = TRUE, cex = 1, seg.len=1, bty = 'n')
```

The figure belies the extra mass computed for the probability at zero (cf. Jorgensen 1997, Figure 4.5ff). 

# Tweedie models for '`cm` data

This is a stability check on the data: can we even really generalize an ED like target density?  Do the parameters of the Tweedie fitted model to the data change or stay the same over time (months)?

```{r rolltw, include=TRUE}
library(tweedie)
library(statmod)

oneperiod.fit <- function(y, xi.vec=seq(1.45, 1.775, by=0.005))
{
  out <- tweedie.profile(y~1, xi.vec=xi.vec, 
                         do.plot=FALSE, do.smooth=FALSE,
                         glm.control(maxit=100),
                         method="series")
  return(list(phi = out$phi.max, xi = out$xi.max))
}


# Do explicit parallelization here over nodes.  
# Note this can be made more elegant with the `parallel` package.

library(snow)

nodes <- 8 # Number of cores to use in parallel
cl <- makeCluster(spec=nodes, type="SOCK")
clusterEvalQ(cl, library(tweedie))
clusterSetupRNGstream(cl)
#clusterExport(cl, "xi.vec")
clusterExport(cl, "oneperiod.fit")

# Now fit them all at once!
system.time(all.tw <- clusterApplyLB(cl, sbdata, oneperiod.fit))

stopCluster(cl)
                    
```

# Time series parameter summaries

Plot summaries of the key parameters of the Tweedie models for each time period (month).

```{r summs, include=TRUE, fig.height=6, fig.keep='last'}
# Now summarize the results
all.tw.sum <- matrix(unlist(all.tw), ncol=2, byrow = TRUE)

# Now make some plots to see what changes over time

twp <- cbind(roll.ged.sb, all.tw.sum)
colnames(twp) <- c("Mean", "Std Dev.", "phi", "p")

# Make a plot to see what changes over time
plot(twp, main="")

```

Now plot the probability of a zero count from the Tweedie, Poisson, and data for comparison.

```{r zeropred, echo=TRUE, fig.width=6, fig.keep='last'}

# Convert using Dunn and Smyth formula
twp.convert <- sapply(1:nrow(twp), 
                      function(i) tweedie.convert(xi = twp[i,"p"], 
                                                  mu=twp[i,"Mean"], 
                                                  phi=twp[i,"phi"]),
                     USE.NAMES = FALSE)

twp.convert <- as.data.frame(matrix(unlist(twp.convert), 
                                    ncol=6, byrow=TRUE))
colnames(twp.convert) <- c("poisson.lambda", 
                           "gamma.scale", "gamma.shape",
                           "p0", "gamma.mean", "gamma.phi")

# Compute the observed zeros proportion by month and plot against the predictions:
freq.zeros <- aggregate(dfs$ged_sb==0,
                        by=list(dfs$month_id), mean)

# Poisson predictions make no sense
# range(dpois(0, lambda=roll.ged.sb[,1]))

plot(ts(cbind(twp.convert$p0, freq.zeros[,2]), 
        start=c(1990,1), freq=12),
     plot.type="single", lwd=2, lty=2:3, col=2:1, 
     ylab = "Pr(ged_sb=0)")

legend(2013, 0.925, 
       c("Observed Frequency", "Tweedie Prediction"), col=2:1,
       lty=2:3, lwd=2, cex=0.75)

```


# Simple prediction models

This section shows how to set up some simple baseline prediction models for each country-month in the data.

### Basic count regressions

This section sets up a series of count regression models.  It assumes you split the data into training-test splits and fit the following models:

- P = Poisson 
- NB = Negbin
- ZIP = ZI Poisson
- ZINB = ZI Negbin
- TW = Tweedie 

This is all done via the `count.cf` function defined below.  The result is a set of `N` simulated draws from the forecast density for each model and outcome to be predicted in the test data.

```{r basiccounts, echo=TRUE, message=FALSE}
# Load needed libraries
library(statmod)
library(tweedie)
library(MASS)
library(pscl)
library(zoo)

set.seed(145)


# count.cf -- fits a single count model to a series and validates it
# against a training / test setup with forecast metrics
#
# Fits each count regression model on train-test data for
# P = Poisson 
# NB = Negbin
# ZIP = ZI Poisson
# ZINB = ZI Negbin
# TW = Tweedie 
#
# After MLE fits, generates and returns N draws from the prediction
# density for each of these models
#

count.cf <- function(train, test, xi.vec = seq(1.1, 1.75, by=0.025), 
                     N = 1000)
{
  # Fit models
  p <- glm(y ~ geo, family = poisson, data=train)
  nb <- glm.nb(y ~ geo, data=train)
  zip <- zeroinfl(y ~ geo, data=train, dist="poisson", model=FALSE, y = FALSE)
  zinb <- zeroinfl(y ~ geo, data=train, dist="negbin", model=FALSE, y = FALSE)
  #  zgeom <- zeroinfl(y ~ 1, data=train, dist="geometric", model=FALSE, y = FALSE)
  
  cat("Fitting Tweedie model grid for xi:\n")  
  param.tw <- tweedie.profile(y ~ geo, xi.vec=xi.vec, 
                              data = train,
                              do.plot=FALSE,
                              control=list(maxit=50),
                              method="series", verbose=FALSE)
  tw <- glm(y ~ geo, data = train,
            family=tweedie(var.power=param.tw$xi.max, link.power=0),
            control=list(maxit=50))
  
  # Predict forecasts for test periods
  n <- nrow(test)
  p.p <- predict(p, newdata=test, type="response")
  p.nb <- predict(nb, newdata=test, type="response")  
  p.zip <- predict(zip, newdata=test, type="response")
  p.zinb <- predict(zinb, newdata=test, type="response")
  #  p.zgeom <- predict(zgeom, newdata=test, type="response")
  p.tw <- predict(tw, newdata=test, type = "response")
  
  # Now simulate the forecasts for each test period
  cat("Sampling forecast densities")
  p.sample <- t(sapply(1:n, function(i) {rpois(N, lambda=p.p[i])}))  
  cat(" .")
  nb.sample <- t(sapply(1:n, function(i) {rnbinom(N, size=nb$theta, mu=p.nb[i])}))
  cat(" .")
  zip.sample <- t(sapply(1:n, function(i) {rpois(N, lambda=p.zip[i])}))
  cat(" .")
  zinb.sample <- t(sapply(1:n, function(i) {rnbinom(N, size=zinb$theta, mu=p.zinb[i])}))  
  cat(" .\n")
  tw.sample <- t(sapply(1:n, function(i) {rtweedie(N, xi=param.tw$xi.max,
                                                   mu=p.tw[i], phi=1) }))
  
  # Some cleanups
  rm(p,nb,zip,zinb,tw,
     p.p,p.nb,p.zip,p.zinb,p.tw)
  
  return(list(P = p.sample,
              NB = nb.sample,
              ZIP = zip.sample,
              ZINB = zinb.sample,
              TW = tw.sample))
}

```

## Simple demo of the count model comparisons

```{r countdemo1, echo=TRUE, warning=FALSE}

# Define a DV as a simple variable and a factor for the countries
dfs$y <- dfs$ged_sb
dfs$geo <- as.factor(dfs$country_id)

# Splits and subsets
train <- dfs[dfs$Year>2020 & dfs$Year < 2024,]
test <- dfs[dfs$Year>2023,]

# Now fit models over the subsets
system.time(count.out <- count.cf(train, test, N=1000))

```


### Count regressions with GAM / splines / GLMMs

This section demonstrates the approach taken in Brandt (2024).   Here an illusration of a basic model with GAM / GLMM components are illustrated and then compared to the earlier section's analyses.

# Scoring forecasts

Here's how one can compute scoring rules to rank the forecasts from each of the models.  This is akin to Brandt (n.d.).

### Scoring with simple metrics

- RMSE
- Divergence
- Brier Scores
- AUROC
- CRPS

### CRPS

```{r scoringdemo, echo=TRUE, message=FALSE, warning=FALSE}
library(scoringutils)

# Get the mean CRPS values across the country-year-draws
check.crps <- lapply(lapply(count.out, 
                                  crps_sample, 
                                  observed = test$y), mean)

# Show the CRPS for the test data
print(unlist(check.crps), digits = 3)

```

## Metrics over time horizons

## Benchmarks comparisons

## Skill scores

# `cm` models with covariates

## Climate

## Demography

## Civil-Military Expenditures


<!-- # PRIO-Grid Month (`pgm`) Data setup -->

<!-- ## Analyses like above for some `pgm` -->
